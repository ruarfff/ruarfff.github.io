{"componentChunkName":"component---src-templates-blog-post-js","path":"/slack-sentiment/","result":{"data":{"site":{"siteMetadata":{"title":"Ruairí's Blog"}},"markdownRemark":{"id":"136f235b-8c30-5131-b91c-b26857b67c25","excerpt":"Sentiment a thought, opinion, or idea based on a feeling about a situation, or a way of thinking about something Sentiment analysis, in the context of this…","html":"<p><a href=\"https://dictionary.cambridge.org/dictionary/english/sentiment\">Sentiment</a></p>\n<blockquote>\n<p>a thought, opinion, or idea based on a feeling about a situation, or a way of thinking about something</p>\n</blockquote>\n<p><a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\">Sentiment analysis</a>, in the context of this article, is the use of natural language processing to quantify the general opinion or mood of some text. We will keep it simple and only look at determining if some text is positive or negative. Positive being generally complimentary in nature. Negative being generally critical in nature. To add some level of (perhaps impractical) practicality we will look at analysing text from a <a href=\"https://slack.com/\">Slack</a> channel.</p>\n<p>In this article we will:</p>\n<ul>\n<li>Download chat history from a public slack channel (which is optional as you can really use any text)</li>\n<li>Use some python tools to process and inspect the chat data</li>\n<li>Use a pre-trained model to classify sentences from the chat as positive or negative</li>\n<li>Train our own custom model with some training data</li>\n<li>Test our own model and use it on the slack data</li>\n<li>Prepare a docker image with our model so we can deploy and use it as a service</li>\n</ul>\n<p>If you just want to look at some code, <a href=\"https://github.com/ruarfff/slack-sentiment\">this GitHub repository</a> has everything. The <a href=\"https://jupyter.org\">jupyter notebook</a> in it also includes a lot of the instructions from here. In this article we will look at all the steps in excruciating detail so we really understand how it all works.</p>\n<h2>Setup</h2>\n<p>The easiest thing to do it the clone <a href=\"https://github.com/ruarfff/slack-sentiment\">this GitHub repository</a>. It is using <a href=\"https://packaging.python.org/tutorials/managing-dependencies/#managing-dependencies\">pipenv</a> to manage dependencies so if you follow the <a href=\"https://github.com/ruarfff/slack-sentiment/blob/main/README.md\">Readme</a> and you have a recent version of <a href=\"https://www.python.org\">python</a> as well as <a href=\"https://pipenv-fork.readthedocs.io/en/latest/\">pipenv</a> installed, you should be good to go.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone git@github.com:ruarfff/slack-sentiment.git\n\n<span class=\"token builtin class-name\">cd</span> slack-sentiment\n\npipenv <span class=\"token function\">sync</span> --dev\n\npipenv run notebook</code></pre></div>\n<p>The example is mostly run in a <a href=\"https://jupyter.org\">jupyter notebook</a>. Of course feel free to use your own setup instead and copy code from here as needed. If you don’t want to use <code class=\"language-text\">pipenv</code> you can also install all the dependencies listed in the Pipfile using pip directly.</p>\n<p>e.g.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">pip <span class=\"token function\">install</span> jupyter slack-sdk nltk numpy pandas matplotlib</code></pre></div>\n<h2>Get Data from a Slack Channel</h2>\n<p>I know I don’t need to say this but obviously be careful and understand the sensitivity of your slack data. I ran this against a slack channel with friends. I used slack as I thought it was a nice example for taking data from an arbitrary source and processing it. It was also a fun example to use.</p>\n<p>I won’t go into too much detail here as the <a href=\"https://api.slack.com\">slack documentation</a> is good but here’s a brief explanation of how to get the chat history for a channel.</p>\n<p>You will need to get the ID for the channel and a token to make a request to the slack api.</p>\n<p><strong>To get the slack channel ID:</strong></p>\n<p>There is probably an easier way but the one I use is to right click the channel name in the channel menu and select “Copy link”. The link will look something like <code class=\"language-text\">https://your-org.slack.com/archives/C12345</code>. The channel ID is the last piece. That’s <code class=\"language-text\">C12345</code> in the example URL.</p>\n<p><strong>To get a token:</strong></p>\n<p>Setup a bot user following these instructions <a href=\"https://api.slack.com/bot-users\">https://api.slack.com/bot-users</a>.\nThe OAuth scope must allow <code class=\"language-text\">channels:history</code>. Add the bot user to the channel you wish to read messages from. The channel should be public. I have not tested any of this with private channels.</p>\n<p>The next section assumes you have 2 environment variables:</p>\n<p><code class=\"language-text\">SLACK_SENTIMENT_BOT_TOKEN</code> - The bot token we setup previously.</p>\n<p><code class=\"language-text\">SLACK_SENTIMENT_CHANNEL_ID</code> - The channel ID we retrieved previously.</p>\n<p>If you are not familiar with how to setup environment variables, feel free to just replace those values with hardcoded values but obviously be careful not to accidentally push your token up to Github or something like that :)\nIncidentally, I do have a <a href=\"https://dev.to/ruarfff/managing-local-app-secrets-and-sharing-secrets-with-your-team-34m1\">post on managing secrets while developing</a> if you’re interested.</p>\n<p>The following python example gets all the slack messages for a channel and prints them:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Get a timestamp for 7 days ago in a weird way. I need to lean more python.</span>\n<span class=\"token comment\"># This is only used if we want to limit how far back in time we go for fetching slack messages.</span>\n<span class=\"token keyword\">from</span> datetime <span class=\"token keyword\">import</span> timedelta<span class=\"token punctuation\">,</span> datetime<span class=\"token punctuation\">,</span> date\ntoday <span class=\"token operator\">=</span> date<span class=\"token punctuation\">.</span>today<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nweek_ago <span class=\"token operator\">=</span> today <span class=\"token operator\">-</span> timedelta<span class=\"token punctuation\">(</span>days<span class=\"token operator\">=</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span>\nweek_ago_timestamp <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>datetime<span class=\"token punctuation\">.</span>combine<span class=\"token punctuation\">(</span>week_ago<span class=\"token punctuation\">,</span> datetime<span class=\"token punctuation\">.</span><span class=\"token builtin\">min</span><span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> datetime<span class=\"token punctuation\">(</span><span class=\"token number\">1970</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> timedelta<span class=\"token punctuation\">(</span>seconds<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">import</span> os\n<span class=\"token comment\"># Import WebClient from Python SDK (github.com/slackapi/python-slack-sdk)</span>\n<span class=\"token keyword\">from</span> slack_sdk <span class=\"token keyword\">import</span> WebClient\n<span class=\"token keyword\">from</span> slack_sdk<span class=\"token punctuation\">.</span>errors <span class=\"token keyword\">import</span> SlackApiError\n<span class=\"token keyword\">import</span> json\n<span class=\"token keyword\">from</span> time <span class=\"token keyword\">import</span> sleep\n\ntoken <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token string\">\"SLACK_SENTIMENT_BOT_TOKEN\"</span><span class=\"token punctuation\">)</span>\nchannel_id <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token string\">\"SLACK_SENTIMENT_CHANNEL_ID\"</span><span class=\"token punctuation\">)</span>\npage_size <span class=\"token operator\">=</span> <span class=\"token number\">100</span>\n\nclient <span class=\"token operator\">=</span> WebClient<span class=\"token punctuation\">(</span>token<span class=\"token operator\">=</span>token<span class=\"token punctuation\">)</span>\n\nconversation_history <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nhas_more <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\ncursor <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> has_more<span class=\"token punctuation\">:</span>\n        has_more <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n        result <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>conversations_history<span class=\"token punctuation\">(</span>\n            channel<span class=\"token operator\">=</span>channel_id<span class=\"token punctuation\">,</span>\n            limit<span class=\"token operator\">=</span>page_size<span class=\"token punctuation\">,</span>\n            cursor<span class=\"token operator\">=</span>cursor\n            <span class=\"token comment\"># If you wanted to limit to the last 7 days uncomment the next line and put a , after cursor on the previous line</span>\n            <span class=\"token comment\"># oldest = week_ago_timestamp</span>\n        <span class=\"token punctuation\">)</span>\n        conversation_history<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">[</span><span class=\"token string\">\"messages\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        has_more <span class=\"token operator\">=</span> result<span class=\"token punctuation\">[</span><span class=\"token string\">'has_more'</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">if</span> result<span class=\"token punctuation\">[</span><span class=\"token string\">'response_metadata'</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Still fetching'</span><span class=\"token punctuation\">)</span>\n            cursor <span class=\"token operator\">=</span> result<span class=\"token punctuation\">[</span><span class=\"token string\">'response_metadata'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'next_cursor'</span><span class=\"token punctuation\">]</span>\n            sleep<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Avoid being rate limited</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Done!'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> SlackApiError <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Error getting conversations: {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>e<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>conversation_history<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Prepare the Data</h2>\n<p>In data engineering a huge amount of time is spent organising and cleaning data. Slack is just one example of a wide variety of extremely versatile data sources. In the following example we will gently dip our toes into that kind of work. Let’s take our slack messages and clean them up a bit. Let’s also break them up into sentences and words.</p>\n<p>I will just drop our main data cleaning function in here and then go through it line by line.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> nltk\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>tokenize <span class=\"token keyword\">import</span> word_tokenize<span class=\"token punctuation\">,</span> sent_tokenize\n<span class=\"token keyword\">import</span> itertools\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_test_data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    all_text <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>message<span class=\"token punctuation\">[</span><span class=\"token string\">'text'</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> message <span class=\"token keyword\">in</span> conversation_history <span class=\"token keyword\">if</span> <span class=\"token string\">'text'</span> <span class=\"token keyword\">in</span> message<span class=\"token punctuation\">]</span>\n    tokenized_text <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>sent_tokenize<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> text <span class=\"token keyword\">in</span> all_text<span class=\"token punctuation\">]</span>\n    sentences <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>itertools<span class=\"token punctuation\">.</span>chain<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>tokenized_text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    tokenized_words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>word_tokenize<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> sentence <span class=\"token keyword\">in</span> sentences<span class=\"token punctuation\">]</span>\n    words <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>itertools<span class=\"token punctuation\">.</span>chain<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>tokenized_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">,</span> words<span class=\"token punctuation\">)</span>\n\nmain_sentences<span class=\"token punctuation\">,</span> main_words <span class=\"token operator\">=</span> get_test_data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>Line by line</strong></p>\n<p><code class=\"language-text\">all_text = [message[&#39;text&#39;] for message in conversation_hissory if &#39;text&#39; in message]</code></p>\n<p>The response from the Slack API stores the text from the slack conversation in a field called text. We don’t want any of the other information like the user who posted it or the time it was posted. That line iterates over each response and extracts the text to a new list.</p>\n<p><code class=\"language-text\">tokenized_text = [sent_tokenize(text) for text in all_text]</code></p>\n<p>Iterate over all the text and use NLTK’s sentence tokeniser to split the text up into sentences.</p>\n<p><code class=\"language-text\">sentences = list(itertools.chain(*tokenized_text))</code></p>\n<p>The previous line actually gave us a list of lists since <code class=\"language-text\">sent_tokenize</code> returns a list. This line flattens that to a single list.</p>\n<p><code class=\"language-text\">tokenized_words = [word_tokenize(sentence) for sentence in sentences]</code></p>\n<p>This line used NLTK’s word tokeniser to split each slack message up into words.</p>\n<p><code class=\"language-text\">words = list(itertools.chain(*tokenized_words))</code></p>\n<p><code class=\"language-text\">word_tokenize</code> returns a list so we ended up with a list of lists. This line flattens that to a single list.</p>\n<p><code class=\"language-text\">return (sentences, words)</code></p>\n<p>Here we are returning a <a href=\"https://docs.python.org/3.3/library/stdtypes.html?highlight=tuple#tuple\">tuple</a> containing the sentences and words. In this case, a tuple is just a handy way to return multiple things from a function without going to the bother of creating a new data type for it.</p>\n<p><code class=\"language-text\">main_sentences, main_words = get_test_data()</code></p>\n<p>We then assign the values from the returned tuple to 2 new variables. We will use these variables again throughout the rest of this example.</p>\n<h2>Use a Pre-trained Model</h2>\n<p>We will be training our own model and we will look at how to deploy our own trained model soon. First it could be useful to look at an example of using a pre-trained model. We will use NLTK’s <a href=\"https://www.nltk.org/api/nltk.sentiment.html\">SentimentIntensityAnalyzer</a>.</p>\n<p>You can import and setup the model like so:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk <span class=\"token keyword\">import</span> download\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>sentiment <span class=\"token keyword\">import</span> SentimentIntensityAnalyzer\n\n<span class=\"token comment\"># The SentimentIntensityAnalyzer model needs us to pull down the vader_lexicon</span>\ndownload<span class=\"token punctuation\">(</span><span class=\"token string\">'vader_lexicon'</span><span class=\"token punctuation\">)</span>\n\nsia <span class=\"token operator\">=</span> SentimentIntensityAnalyzer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Then you can take one of the sentences from our slack “corpus” and pass it in:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">sia<span class=\"token punctuation\">.</span>polarity_scores<span class=\"token punctuation\">(</span>main_sentences<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"compound\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>That will return a number. If the number is greater than 0 then we say the sentence has a positive sentiment.</p>\n<p>Here’s an example that checks each sentence and draws a pie chart showing the ratio of positive vs negative.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">num_pos <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nnum_neg <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">is_positive</span><span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">bool</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> sia<span class=\"token punctuation\">.</span>polarity_scores<span class=\"token punctuation\">(</span>sentence<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"compound\"</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span>\n\n<span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> main_sentences<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> is_positive<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        num_pos <span class=\"token operator\">+=</span><span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        num_neg <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n\nlabels <span class=\"token operator\">=</span> <span class=\"token string\">'Positive'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Negative'</span>\nsizes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>num_pos<span class=\"token punctuation\">,</span> num_neg<span class=\"token punctuation\">]</span>\n\nfig1<span class=\"token punctuation\">,</span> ax1 <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>subplots<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nax1<span class=\"token punctuation\">.</span>pie<span class=\"token punctuation\">(</span>sizes<span class=\"token punctuation\">,</span> labels<span class=\"token operator\">=</span>labels<span class=\"token punctuation\">,</span> autopct<span class=\"token operator\">=</span><span class=\"token string\">'%1.1f%%'</span><span class=\"token punctuation\">,</span>\n        shadow<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> startangle<span class=\"token operator\">=</span><span class=\"token number\">90</span><span class=\"token punctuation\">)</span>\nax1<span class=\"token punctuation\">.</span>axis<span class=\"token punctuation\">(</span><span class=\"token string\">'equal'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>Example output:</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 333px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4607b4e1790fd5a65197a704f1b44c75/24c7e/sent-example-pie.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.62025316455697%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACLUlEQVQoz22Tz2sTQRTHh0b04FXwoPUHmqZNYkARwYMH/wI9SwUjqHhJaLGb2TSbjfFgeylUFFKkB8GimFSTgLSk/gKliIo92AZ7CLb5QaQSbTXZZHdn5pnZTciP9rGws/v4zPf73rxB0BmsudAI1dQaMNL4zRhsC8QzrJHRKX9/zv2+HvtqnVycmRDg4UWSmoA/eZ6mZAfYDAOEsberPULc4n2Ghl8+EAfBv18fOQh3z8D3N4YD2gHXVVVVrdbU+se996vIE7XgxB5/AonzEckNoWN6+CQEjoM8APlvXTwihKxns4VcdmWtsE9KIl+ix5+04DjC85GAG+SjuuyEsAvwIZgeNGDWYZsYju+/SyNvdJfnEboxZRFmEZ6bClxpwLKDK4ccUEy3ixuwsdm1J4toaNYiJpAvvhs/R3ghErjahO1w+wT4eiG90N65Fux+/IHD3PMLhFPnxMmN4GmQ+5nsYEE7M+GVFOuGDdvjc0vIG+Otwq8u+Me2gi6Q++pWy2L/D4+1JNi47cJyt21qKC+vF/eOxBB+fXNUZEEblQcoL9XOn7CL4cMwfennxq9MJpPN5jRNa52zyQ9FP97yuiFsJSEnMUvl1bpAsrFRK6x9URn829qsVBRKaQs2Z6xS08ozw7rQy8QjIFlB6uMn7DtAgk59KbHDkLSmmrG6vkJo6VNyM3K5fOesEjqljJ9XngpqPm02mTWjG26/ABqltb+laqmoV8u0uff2i/EfZHqTptSp0HEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Alt Text\"\n        title=\"Alt Text\"\n        src=\"/static/4607b4e1790fd5a65197a704f1b44c75/24c7e/sent-example-pie.png\"\n        srcset=\"/static/4607b4e1790fd5a65197a704f1b44c75/c26ae/sent-example-pie.png 158w,\n/static/4607b4e1790fd5a65197a704f1b44c75/6bdcf/sent-example-pie.png 315w,\n/static/4607b4e1790fd5a65197a704f1b44c75/24c7e/sent-example-pie.png 333w\"\n        sizes=\"(max-width: 333px) 100vw, 333px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>This was just to show an example but it might also demonstrate the importance of the next step. The results we got from the data in its current state may not have been very accurate. Let’s see what we can do to clean up the data and reduce the noise in there.</p>\n<h2>Clean the Data</h2>\n<p>From this point on we won’t be using the sentences from our dataset anymore. We will focus on the individual words instead. Let’s create a frequency distribution graph of the words. This is also to demonstrate how many of them are more or less useless.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>probability <span class=\"token keyword\">import</span> FreqDist\n\nfdist <span class=\"token operator\">=</span> FreqDist<span class=\"token punctuation\">(</span>main_words<span class=\"token punctuation\">)</span>\n\nfdist<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>cumulative<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 472px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ba090e062de841af01e288c0375f4f48/3c5de/sent-word-plot.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60.12658227848101%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABJ0lEQVQoz5WRi26DMAxF+/+fOE1oKqVNSEIeJg/evYGug2qdtCtjIcixr53TsizDMDDGjHPGOmfxvJVd/8YYl1WnFR5rzm6Nk86nkBXfyHu/5T08gGWadBv6LkHdG4FERvUdPI6cM2lJkO9S/AMGdoCnaVJKYWYfwkW3/b/gcRyFEJzzGELZkCbwKcRcI5c5CtNu+cc2eHTu8umuqI3zcRqGvu+fDdN3vHZ+LAydYwQB84Uw0rWasNiEAoihfwQ2gnyA53lGZ3zK3eA5JWY9t/6q6dLkEM4rCojakPWYb2cbghnc/jY8bk2KWgkhlUSqhSjKW1Gxz/L68VWer3k7B3gTYCICb6y9rcI71nM5nz25lpzRjTUaTn+BXySlqqoKdznN8xPY6w5JMbRoId8V7wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Alt Text\"\n        title=\"Alt Text\"\n        src=\"/static/ba090e062de841af01e288c0375f4f48/3c5de/sent-word-plot.png\"\n        srcset=\"/static/ba090e062de841af01e288c0375f4f48/c26ae/sent-word-plot.png 158w,\n/static/ba090e062de841af01e288c0375f4f48/6bdcf/sent-word-plot.png 315w,\n/static/ba090e062de841af01e288c0375f4f48/3c5de/sent-word-plot.png 472w\"\n        sizes=\"(max-width: 472px) 100vw, 472px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>If you run that, you probably see a lot of words like “I”, “if”, “me” etc. Also maybe some punctuations. These are not too useful for our analysis.</p>\n<p>Let’s look at a helper function to clean that up.</p>\n<p>When we start training our own model we will be using a method where we store each word along with its frequency and train a model using those features (the words and frequency) along with labels (positive and negative).</p>\n<p>We will need 2 functions. One to clean our data, removing noise such as meaningless words, punctuation etc. Then we need a function to convert a stream of words into a hashmap with words and number of occurences.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Assumes you previously had from nltk import download</span>\ndownload<span class=\"token punctuation\">(</span><span class=\"token string\">'stopwords'</span><span class=\"token punctuation\">)</span>\ndownload<span class=\"token punctuation\">(</span><span class=\"token string\">'names'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> stopwords<span class=\"token punctuation\">,</span> names\n<span class=\"token keyword\">from</span> string <span class=\"token keyword\">import</span> punctuation\n\n<span class=\"token comment\"># A large set of names. We lowercase here as we will lowercase our own data first too</span>\nname_words <span class=\"token operator\">=</span> <span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>n<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> n <span class=\"token keyword\">in</span> names<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># A set of stopwords provided by NLTK (if, as etc.)</span>\nstop_words <span class=\"token operator\">=</span> <span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>stopwords<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token string\">\"english\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">clean_words</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>w <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> words <span class=\"token keyword\">if</span> w<span class=\"token punctuation\">.</span>isalpha<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">if</span> w <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> stop_words <span class=\"token keyword\">and</span> w <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> name_words <span class=\"token keyword\">and</span> w <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> punctuation<span class=\"token punctuation\">]</span></code></pre></div>\n<p>Let’s split <code class=\"language-text\">clean_words</code> and look at each bit.</p>\n<p><code class=\"language-text\">w for w in [w.lower() for w in words if w.isalpha()]</code></p>\n<p>Go through all the words in the words array which is passed as an argument to our function. Check if a word is alphanumeric (no special characters we can’t use). Then extract that out to a new list.</p>\n<p><code class=\"language-text\">if w not in stop_words and w not in name_words and w not in punctuation</code></p>\n<p>This filters our list. We filter out all those stop words. We filter out any names and punctuations. This gives us a cleaner set to work with.</p>\n<p>You may be wondering what we are doing here? How can we analyse the data without context? We are going to use a pretty basic model. We will more or less be taking bags of words with labels and training a model with them. Then we will feed other bags of words to the model and have it make a guess at the sentiment. It doesn’t know anything about actual sentences. It mostly goes off of word frequency.</p>\n<p>To help us shape our data even more for later use we need one more function.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">word_counts</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    counts <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> words<span class=\"token punctuation\">:</span>\n        counts<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> counts<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">return</span> counts</code></pre></div>\n<p>This takes our words and puts them into a HashMap where the key is the word and the value is the number of times that word has shown up in our data.</p>\n<p>Now we can get the data ready.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">main_words <span class=\"token operator\">=</span> clean_words<span class=\"token punctuation\">(</span>main_words<span class=\"token punctuation\">)</span>\nmain_words_counts <span class=\"token operator\">=</span> word_counts<span class=\"token punctuation\">(</span>main_words<span class=\"token punctuation\">)</span></code></pre></div>\n<p>You can create a new distribution graph and see if it looks better.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">fdist <span class=\"token operator\">=</span> FreqDist<span class=\"token punctuation\">(</span>main_words<span class=\"token punctuation\">)</span>\nfdist<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span>cumulative<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 419px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c3a150f1b0ffd0bc1eaac152367cd581/d587d/sent-word-plot-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.15189873417721%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAABg0lEQVQoz4WS63KCMBCFff9HK4jwR8dROq0iN4FAyIUQwB4IWm213RmWhOXby0kWl8tFKVWWZZARyjhnL41fg5RSeIALPCCLPC+YPBCORO0L67rOLJqmeYDzohi6NqU8KVmv20Y9Ma21WUgpv+G6rvM8Rz6h1D4tayGVan6bqTn+JsQMY1NMhnyXXmcVD0k96PE/+S+cJGmSJIQQE0ZJFE8qBrJrpyb/gI3UqHwLc9lEhCFFUFAmZHvln8B4sDIzmzAGRk0hm6LmflpllENCNR2DnGZ5gEe17+DbtBAXK7SQUVYx0ekxRzt+vFMbpw8YZN/38DiS2Wvdd10lmhNhIeFByQ9FndWCijsYOp/P5ziO4SFeFEXw2IZhGJxOaZzkCEDW9ByE0ebjuHn/xCUb4WEYDO+6rm3bjuOs12vLsjzP2+122+3WXi7fLNv1vJXjuKsVXu/+Hn3NlWG4emi+qip4JEJNzGxCvu+jC2zNlUZT0M6EZviHmXaQ63A8Zln2NAr7AuDHILyx5rVLAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Alt Text\"\n        title=\"Alt Text\"\n        src=\"/static/c3a150f1b0ffd0bc1eaac152367cd581/d587d/sent-word-plot-2.png\"\n        srcset=\"/static/c3a150f1b0ffd0bc1eaac152367cd581/c26ae/sent-word-plot-2.png 158w,\n/static/c3a150f1b0ffd0bc1eaac152367cd581/6bdcf/sent-word-plot-2.png 315w,\n/static/c3a150f1b0ffd0bc1eaac152367cd581/d587d/sent-word-plot-2.png 419w\"\n        sizes=\"(max-width: 419px) 100vw, 419px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Training Our Own Model</h2>\n<p>Now we will train a model so we can call ourselves AI scientists. We will use something called a <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\">Naive Bayes classifier</a>. NLTK provides one for us to use. Our main job here really is to provide a lot of labelled data to train the model and the library does the rest. Over time as we become more familiar with the data we might figure out clever ways of modifying it to improve our model. We will look briefly at that concept but will not go into it in any depth here.</p>\n<p>In <a href=\"https://github.com/ruarfff/slack-sentiment\">the sample repo</a> for this there are some data files. These can also be downloaded from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\">Sentiment Labelled Sentences Data Set</a>. We will also use some data provided by NLTK.</p>\n<p>First we load the files from the repo using the <a href=\"https://pandas.pydata.org\">pandas library</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\namazon <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'sentiment_labelled_sentences/amazon_cells_labelled.txt'</span><span class=\"token punctuation\">,</span> names<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> sep<span class=\"token operator\">=</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span>\nimdb <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'sentiment_labelled_sentences/imdb_labelled.txt'</span><span class=\"token punctuation\">,</span> names<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> sep<span class=\"token operator\">=</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span>\nyelp <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'sentiment_labelled_sentences/yelp_labelled.txt'</span><span class=\"token punctuation\">,</span> names<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> sep<span class=\"token operator\">=</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Those files must be located in a folder called <code class=\"language-text\">sentiment_labelled_sentences</code> in the same directory as the python script if using the code as is.</p>\n<p>Now we download the movie review corpus provided by NLTK.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Assumes you previously had from nltk import download</span>\ndownload<span class=\"token punctuation\">(</span><span class=\"token string\">'movie_reviews'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The next step is to take all that data and split it into 2 lists. One list of negative reviews. The other of positive reviews.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">reviews <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token operator\">*</span>amazon<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>imdb<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>yelp<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">]</span>\nsentiment <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token operator\">*</span>amazon<span class=\"token punctuation\">[</span><span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>imdb<span class=\"token punctuation\">[</span><span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>yelp<span class=\"token punctuation\">[</span><span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">]</span>\n\npositive_reviews <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nnegative_reviews <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> movie_reviews\n\n<span class=\"token comment\"># Process the corpus data which is split into files with IDs of pos and neg</span>\n<span class=\"token keyword\">for</span> f <span class=\"token keyword\">in</span> movie_reviews<span class=\"token punctuation\">.</span>fileids<span class=\"token punctuation\">(</span><span class=\"token string\">'pos'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    positive_reviews<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>word_counts<span class=\"token punctuation\">(</span>clean_words<span class=\"token punctuation\">(</span>movie_reviews<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'pos'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> f <span class=\"token keyword\">in</span> movie_reviews<span class=\"token punctuation\">.</span>fileids<span class=\"token punctuation\">(</span><span class=\"token string\">'neg'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    negative_reviews<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>word_counts<span class=\"token punctuation\">(</span>clean_words<span class=\"token punctuation\">(</span>movie_reviews<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'neg'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Process the data we extracted from files into reviews and sentiment lists with matching indexes</span>\n<span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> r <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>reviews<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    review_words <span class=\"token operator\">=</span> word_tokenize<span class=\"token punctuation\">(</span>r<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> sentiment<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n        positive_reviews<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>word_counts<span class=\"token punctuation\">(</span>clean_words<span class=\"token punctuation\">(</span>review_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'pos'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        negative_reviews<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>word_counts<span class=\"token punctuation\">(</span>clean_words<span class=\"token punctuation\">(</span>review_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'neg'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Now that we have all that data prepared we can train a model.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>classify <span class=\"token keyword\">import</span> NaiveBayesClassifier\n\n<span class=\"token comment\"># We will use 80% of the data set for training and 20% for testing</span>\nsplit_pct <span class=\"token operator\">=</span> <span class=\"token number\">.80</span>\n\npos_split <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>positive_reviews<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>split_pct<span class=\"token punctuation\">)</span>\nneg_split <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>positive_reviews<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>split_pct<span class=\"token punctuation\">)</span>\n\ntrain_set <span class=\"token operator\">=</span> positive_reviews<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>pos_split<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> negative_reviews<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>neg_split<span class=\"token punctuation\">]</span>\ntest_set <span class=\"token operator\">=</span> positive_reviews<span class=\"token punctuation\">[</span>pos_split<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> negative_reviews<span class=\"token punctuation\">[</span>neg_split<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n\nmodel <span class=\"token operator\">=</span> NaiveBayesClassifier<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>train_set<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Here we split the data into training and testing sets:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">split_pct <span class=\"token operator\">=</span> <span class=\"token number\">.80</span>\n\npos_split <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>positive_reviews<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>split_pct<span class=\"token punctuation\">)</span>\nneg_split <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>positive_reviews<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>split_pct<span class=\"token punctuation\">)</span>\n\ntrain_set <span class=\"token operator\">=</span> positive_reviews<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>pos_split<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> negative_reviews<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>neg_split<span class=\"token punctuation\">]</span>\ntest_set <span class=\"token operator\">=</span> positive_reviews<span class=\"token punctuation\">[</span>pos_split<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> negative_reviews<span class=\"token punctuation\">[</span>neg_split<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>It’s hard to know what the optimal split is but I have\n<a href=\"https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validation\">read some discussion indicating 80/20 might be a good start</a>.</p>\n<p>Once we have our data read, the actual training of the model is simply calling one function provided by NLTK:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> NaiveBayesClassifier<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>train_set<span class=\"token punctuation\">)</span></code></pre></div>\n<p>At this point we can start using the model to classify all that data we prepared from slack.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>classify<span class=\"token punctuation\">(</span>main_words_counts<span class=\"token punctuation\">)</span></code></pre></div>\n<p>It is a good idea for us to use our test data and determine the accuracy of the model first though.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>classify<span class=\"token punctuation\">.</span>util <span class=\"token keyword\">import</span> accuracy\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span> <span class=\"token operator\">*</span> accuracy<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> test_set<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>When I checked the model it was coming in between 65% and 70% accurate. At this point there’s some work to do to improve the model. I am not going to go into that now except to provide some basic pointers. Perhaps in a follow up article I will get into it. For now let’s look at some things you can do to see where things need improving.</p>\n<p>We can gather all the guesses that went wrong from the test data.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">errors <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> test_set<span class=\"token punctuation\">:</span>\n    guess <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>classify<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> guess <span class=\"token operator\">!=</span> label<span class=\"token punctuation\">:</span>\n        errors<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">,</span> guess<span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>errors<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Looking at the places where guesses went wrong, maybe we can modify the data to improve the training. Maybe take variations on a word and ad that to the training set.</p>\n<p>We can see which features our model believes are most informative.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model<span class=\"token punctuation\">.</span>show_most_informative_features<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>By getting a better understanding of how the model learns, we improve our ability to know how to train it. It’s difficult but also interesting.</p>\n<h2>Deploying a Model</h2>\n<p>Now we can save the model. You could push this model up somewhere and pull it in to some other useful location. For an example of how to deploy this model in a web service using docker please see the <a href=\"https://github.com/ruarfff/slack-sentiment/blob/main/README.md\">Readme for this project</a>.</p>\n<p>You can imagine some process whereby you train a model at some interval or on some event and push the model to a location from which it gets pulled down for use in another application or service.</p>\n<p>Write the model to a file:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pickle\n\nmodel_file <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'.models/sentiment_classifier.pickle'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span>\npickle<span class=\"token punctuation\">.</span>dump<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> model_file<span class=\"token punctuation\">)</span>\nmodel_file<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>For a full example of how to then use that model in a web service please make sure to look at the <a href=\"https://github.com/ruarfff/slack-sentiment\">Example Repository</a>.</p>\n<p>A quick example of how if might work follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model_file <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'.models/sentiment_classifier.pickle'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rb'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> pickle<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>model_file<span class=\"token punctuation\">)</span>\nmodel_file<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span> <span class=\"token operator\">*</span> accuracy<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> test_set<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>show_most_informative_features<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>classify<span class=\"token punctuation\">(</span>main_words_counts<span class=\"token punctuation\">)</span></code></pre></div>\n<p>That’s it. Hopefully this has been useful. Thank you for reading.</p>\n<h2>References</h2>\n<p>I wrote this post as I was studying material from the following places:</p>\n<p><a href=\"https://www.manning.com/liveproject/training-and-deploying-an-ml-model-as-a-microservice\">Training and Deploying an ML Model as a Microservice</a></p>\n<p><a href=\"https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\">Text Analytics for Beginners using NLTK</a></p>\n<p><a href=\"https://realpython.com/python-nltk-sentiment-analysis/\">Sentiment Analysis: First Steps With Python’s NLTK Library</a></p>\n<p><a href=\"https://medium.com/dev-genius/training-an-ml-model-for-sentiment-analysis-in-python-63b6b8c68792\">Training an ML Model for Sentiment Analysis in Python</a></p>","frontmatter":{"title":"Check the Mood of a Slack Channel using Machine Learning","date":"February 10, 2021","description":null}},"previous":{"fields":{"slug":"/tech-stuff-found-2nd-half-2020/"},"frontmatter":{"title":"Tech stuff found in the second half of 2020"}},"next":null},"pageContext":{"id":"136f235b-8c30-5131-b91c-b26857b67c25","previousPostId":"f1690744-cb13-568f-88a2-43e44b8980cc","nextPostId":null}},"staticQueryHashes":["2841359383","916993862"]}